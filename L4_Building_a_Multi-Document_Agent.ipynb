{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b71ff6",
   "metadata": {},
   "source": [
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 201,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff58c52",
   "metadata": {
    "height": 65
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2c6a9f",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\", \"page_numbers\": [\"19\"]}\n",
      "=== Function Output ===\n",
      "The proposed framework achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, demonstrating the effectiveness and generalization of the model.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is not explicitly mentioned in the paper. However, the proposed framework achieved state-of-the-art performance on cross-dataset and cross-manipulation evaluations, demonstrating the effectiveness and generalization of the model.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This framework enables the language model to tailor its behavior to diverse task requirements during the inference phase, significantly outperforming state-of-the-art language models and retrieval-augmented models on various tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve significant context extension with less GPU memory cost and training time compared to full fine-tuning. This approach retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This framework enables the language model to tailor its behavior to diverse task requirements during the inference phase, significantly outperforming state-of-the-art language models and retrieval-augmented models on various tasks.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve significant context extension with less GPU memory cost and training time compared to full fine-tuning. This approach retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This framework enables the language model to tailor its behavior to diverse task requirements during the inference phase, significantly outperforming state-of-the-art language models and retrieval-augmented models on various tasks.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve significant context extension with less GPU memory cost and training time compared to full fine-tuning. This approach retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 473,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    \n",
    "    \n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 150,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 31,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 31,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 268,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 99,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is a combination of HumanEval and MBPP datasets.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of software engineering task instances collected from real GitHub repositories, including issues and corresponding pull requests. It includes task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset is designed to be challenging, covering various software engineering challenges like patch generation, reasoning over long contexts, navigating codebase directories, and capturing dependency-based relationships across modules. The dataset is validated through execution-based verification to ensure usability and fairness for evaluating language models in the context of software engineering tasks.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is a combination of HumanEval and MBPP datasets. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of software engineering task instances collected from real GitHub repositories, including issues and corresponding pull requests. It includes task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset is designed to be challenging, covering various software engineering challenges like patch generation, reasoning over long contexts, navigating codebase directories, and capturing dependency-based relationships across modules. The dataset is validated through execution-based verification to ensure usability and fairness for evaluating language models in the context of software engineering tasks.\n",
      "assistant: The evaluation dataset used in MetaGPT is a combination of HumanEval and MBPP datasets. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of software engineering task instances collected from real GitHub repositories, including issues and corresponding pull requests. It includes task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset is designed to be challenging, covering various software engineering challenges like patch generation, reasoning over long contexts, navigating codebase directories, and capturing dependency-based relationships across modules. The dataset is validated through execution-based verification to ensure usability and fairness for evaluating language models in the context of software engineering tasks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Compare and contrast the approach in the LongLoRA paper.\"}\n",
      "=== Function Output ===\n",
      "The LongLoRA paper introduces an approach that efficiently extends the context length of large language models (LLMs) while minimizing computational costs compared to full fine-tuning. It utilizes a method called LoRA, which adapts pre-trained models to new datasets by updating attention weights. However, the paper shows that LoRA alone is not sufficient for extending LLMs to long contexts. To address this, LongLoRA combines LoRA with Shifted Sparse Attention (S2-Attn) to approximate long contexts efficiently during training while retaining the original attention architecture during inference. This combination results in improved training speed, memory efficiency, and strong empirical results on various tasks and context lengths. Additionally, LongLoRA emphasizes the importance of learnable embedding and normalization layers for successful long context fine-tuning, showcasing its effectiveness in extending context lengths for LLMs.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Compare and contrast the approach in the LoftQ paper.\"}\n",
      "=== Function Output ===\n",
      "The approach in the LoftQ paper introduces a novel quantization framework that combines quantization and low-rank approximation to provide a better initialization for subsequent fine-tuning. This method actively integrates low-rank approximation with quantization to align with the original pre-trained weights, improving generalization in downstream tasks. In contrast, traditional quantization methods like QLoRA primarily focus on quantization techniques and may overlook the importance of subsequent LoRA fine-tuning. QLoRA attaches zero-initialized low-rank adapters to the quantized pre-trained model, potentially leading to performance degradation, especially in low-bit scenarios. LoftQ, on the other hand, jointly optimizes the quantized backbone and low-rank adapters to address quantization discrepancies, resulting in improved performance across various precision levels, particularly excelling in low-bit scenarios. Additionally, LoftQ demonstrates stability and improved performance compared to QLoRA by effectively preserving the starting point of pre-trained weights and achieving convergence in all cases.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an approach that extends the context length of large language models (LLMs) efficiently while minimizing computational costs compared to full fine-tuning. It combines LoRA with Shifted Sparse Attention (S2-Attn) to approximate long contexts efficiently during training and retain the original attention architecture during inference. This combination improves training speed, memory efficiency, and shows strong empirical results on various tasks and context lengths. LongLoRA emphasizes the importance of learnable embedding and normalization layers for successful long context fine-tuning.\n",
      "\n",
      "On the other hand, the LoftQ paper introduces a quantization framework that combines quantization and low-rank approximation to provide a better initialization for subsequent fine-tuning. It integrates low-rank approximation with quantization to align with the original pre-trained weights, improving generalization in downstream tasks. In contrast to traditional quantization methods like QLoRA, LoftQ jointly optimizes the quantized backbone and low-rank adapters to address quantization discrepancies, resulting in improved performance across various precision levels, especially in low-bit scenarios. LoftQ demonstrates stability and improved performance compared to QLoRA by effectively preserving the starting point of pre-trained weights and achieving convergence in all cases.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
